{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be52257e",
   "metadata": {},
   "source": [
    "##Simple Gen AI app using langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e53d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "load_dotenv()\n",
    "#langsmith tracking\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN)TRACKING_V2']='True'\n",
    "os.environ['LANGCHAIN_PROJECT']=os.getenv('LANGCHAIN_PROJECT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b17fd5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data ingestion--from the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader \n",
    "import bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50e378f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1706a010c70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the data source\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"theme-doc-markdown markdown\"))\n",
    "loader = WebBaseLoader(web_paths=(\"https://docs.smith.langchain.com/observability\",),bs_kwargs={\"parse_only\":bs4_strainer})\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175a8803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability'}, page_content='Observability Quick Start\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you\\'re already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependencies\\u200b\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API key\\u200b\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"# The example uses OpenAI, but it\\'s not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n4. Define your application\\u200b\\nWe will instrument a simple RAG\\napplication for this tutorial, but feel free to use your own code if you\\'d like - just make sure\\nit has an LLM call!\\nApplication CodePythonTypeScriptfrom openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";const openAIClient = new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}\\n5. Trace OpenAI calls\\u200b\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}\\nNow when you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the OpenAI call in LangSmith\\'s default tracing project. It should look something like this.\\n\\n6. Trace entire application\\u200b\\nYou can also use the [traceable] decorator (Python or TypeScript) to trace your entire application instead of just the LLM calls.\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});\\nNow if you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this\\n\\nNext steps\\u200b\\nCongratulations! If you\\'ve made it this far, you\\'re well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading and convert it into document\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25540c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 0}, page_content=\"Observability Quick Start\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you're already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependencies\\u200b\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API key\\u200b\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 659}, page_content='2. Create an API key\\u200b\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\\nShellexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=\"<your-langsmith-api-key>\"# The example uses OpenAI, but it\\'s not necessary if your code uses another LLM providerexport OPENAI_API_KEY=\"<your-openai-api-key>\"\\n4. Define your application\\u200b\\nWe will instrument a simple RAG\\napplication for this tutorial, but feel free to use your own code if you\\'d like - just make sure\\nit has an LLM call!'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 1190}, page_content='Application CodePythonTypeScriptfrom openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";const openAIClient = new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 1992}, page_content='= new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 2665}, page_content='5. Trace OpenAI calls\\u200b\\nThe first thing you might want to trace is all your OpenAI calls. LangSmith makes this easy with the wrap_openai (Python) or wrapOpenAI (TypeScript) wrappers.\\nAll you have to do is modify your code to use the wrapped client instead of using the OpenAI client directly.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 2957}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 3756}, page_content='OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 4531}, page_content='Now when you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the OpenAI call in LangSmith\\'s default tracing project. It should look something like this.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 4736}, page_content='6. Trace entire application\\u200b\\nYou can also use the [traceable] decorator (Python or TypeScript) to trace your entire application instead of just the LLM calls.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 4895}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 5685}, page_content='openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 6228}, page_content='Now if you call your application as follows:\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability', 'start_index': 6434}, page_content=\"Next steps\\u200b\\nCongratulations! If you've made it this far, you're well on your way to being an expert in observability with LangSmith.\\nHere are some topics you might want to explore next:\\n\\nTrace multiturn conversations\\nSend traces to a specific project\\nFilter traces in a project\\n\\nOr you can visit the how-to guides page to find out about all the things you can do with LangSmith observability.\\nIf you prefer a video tutorial, check out the Tracing Basics video from the Introduction to LangSmith Course.\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dividing the documents into chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size =1000,chunk_overlap =200,add_start_index =True)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a5d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting chunks into vectors\n",
    "embeddings = (OllamaEmbeddings(model = \"all-minilm\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70224dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the vectors in vectordb (FAISS)\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents=chunks,embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a27f56f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Observability Quick Start\\nThis tutorial will get you up and running with our observability SDK by showing you how to\\ntrace your application to LangSmith.\\nIf you're already familiar with the observability SDK, or are interested in tracing more than just\\nLLM calls you can skip to the next steps section,\\nor check out the how-to guides.\\nTrace LangChain or LangGraph ApplicationsIf you are using LangChain or LangGraph, which both integrate seamlessly with LangSmith,\\nyou can get started by reading the guides for tracing with LangChain or tracing with LangGraph.\\n1. Install Dependencies\\u200b\\nPythonTypeScriptpip install -U langsmith openaiyarn add langsmith openai\\n2. Create an API key\\u200b\\nTo create an API key head to the LangSmith settings page. Then click Create API Key.\\n3. Set up your environment\\u200b\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query from a vectordb\n",
    "query = \"trace your application in langsmith\"\n",
    "result = vectorstore.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a language model\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "llm = OllamaLLM(model =\"llama3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the following question based only on the provided context:\n",
    "    <question>{input}</question>\n",
    "    <context>{context}</context>\"\"\"\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "844430ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='Answer the following question based only on the provided context:\\n    <question>{input}</question>\\n    <context>{context}</context>'), additional_kwargs={})])\n",
       "| OllamaLLM(model='llama3')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a05af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace your application in langsmith, you need to:\n",
      "\n",
      "1. Wrap OpenAI calls using `wrap_openai` (Python) or `wrapOpenAI` (TypeScript)\n",
      "2. Use the `[traceable]` decorator (Python or TypeScript) to trace your entire application instead of just LLM calls.\n",
      "\n",
      "For example, if you want to trace an OpenAI call:\n",
      "\n",
      "```python\n",
      "from openai import OpenAI\n",
      "from langsmith.wrappers import wrap_openai\n",
      "\n",
      "openai_client = wrap_openai(OpenAI())\n",
      "...\n",
      "```\n",
      "\n",
      "Or if you want to trace the entire pipeline:\n",
      "\n",
      "```python\n",
      "@traceable\n",
      "def rag(question):\n",
      "    ...\n",
      "```\n",
      "\n",
      "Note: The code above should be used in your application's `rag` function.\n"
     ]
    }
   ],
   "source": [
    "question = \"How to trace your application in langsmith\"\n",
    "text =document_chain.invoke({\"input\": question, \"context\": chunks})\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f494e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieval chain\n",
    "retriever = vectorstore.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f61d861e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001707E8A6140>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='Answer the following question based only on the provided context:\\n    <question>{input}</question>\\n    <context>{context}</context>'), additional_kwargs={})])\n",
       "            | OllamaLLM(model='llama3')\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d9cb8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the response from the LLM\n",
    "response =retrieval_chain.invoke({\"input\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "909e9bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace your application in Langsmith, you can follow these steps:\n",
      "\n",
      "1. Install the necessary dependencies using pip or yarn.\n",
      "2. Create an API key on the LangSmith settings page and create a tracing project.\n",
      "3. Set up your environment by exporting the `LANGSMITH_TRACING` and `LANGSMITH_API_KEY` variables.\n",
      "\n",
      "When you call your application, such as `rag(\"where did harrison work\")`, this will produce a trace of just the OpenAI call in LangSmith's default tracing project.\n",
      "\n",
      "Note that you'll also need to define your application by instrumenting it with an LLM call.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc363ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
